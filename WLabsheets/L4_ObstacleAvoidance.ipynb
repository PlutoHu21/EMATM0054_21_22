{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L4_ObstacleAvoidance",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyONw6emoQxA2txLBis5tFwO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulodowd/EMATM0053_21_22/blob/main/WLabsheets/L4_ObstacleAvoidance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gawky_WGun5Z"
      },
      "source": [
        "# Labsheet 4: Obstacle Avoidance\n",
        "\n",
        "In previous labsheets we have investigated:\n",
        "- making the robot move.\n",
        "- bang-bang control.\n",
        "- weighted-sensing for line following.\n",
        "- proportional control.\n",
        "- Finite State Machines, transitioning between states or robot behaviours.\n",
        "\n",
        "In this labsheet, we will investigate the new required behaviour Obstacle Avoidance.  This labsheet provides exercises to develop a `reactive` obstacle avoidance controller.  This means that the obstacle avoidance behaviour is created through an immediate response to sensor stimuli, with no memory or planning algorithmically specified.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zWBXXX4A3nb"
      },
      "source": [
        "<hr><br><br><br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzL0dnt4vJ4u"
      },
      "source": [
        "## E-Puck Proximity Sensors\n",
        "\n",
        "Within the provided Webots world file, your simulated robot should meet an obstruction along the line following map.  This obstruction is a pink block.  The exact location of the pink block is not critical, however it should be investigated by you for the challenge is presents:\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/paulodowd/EMATM0053_21_22/blob/main/images/Webots_Obstruction.png?raw=true\">\n",
        "</p>\n",
        "\n",
        "Your simulated robot is required to detect this block and safely navigate around it, to rejoin the line and continue line following.  To achieve this functionality, we will use the proximity sensors of the e-puck robot.\n",
        "\n",
        "The simulated e-puck robot has 8 infra-red proximity sensors, positioned around the robot body.  These sensors are enumerated (indexed) following the same scheme in the below diagram (`ps0` ... `ps7`, \"proximity sensor\"):\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/cyberbotics/webots/released/docs/guide/images/robots/epuck/sensors_and_leds.png\">\n",
        "</p>\n",
        "\n",
        "Similar to the ground sensor, the proximity sensor transmits a beam of infra-red light and measures the intensity of the reflection.  These sensors have been modelled in Webots to have the following **`sensor response`**:\n",
        "\n",
        "<p align=\"center\">\n",
        "<img width=\"75%\" src=\"https://github.com/paulodowd/EMATM0053_21_22/blob/main/images/Webots_EpuckProxResponse.png?raw=true\">\n",
        "</p>\n",
        "\n",
        "In the above plot from the <a href=\"https://cyberbotics.com/doc/guide/epuck#e-puck-model\">Webots documentation</a>, we can observe:\n",
        "- the sensor response is `non-linear`.\n",
        "- the sensor response is `high` at `close proximity`.\n",
        "- the sensor response is `low` at `far proximity`.\n",
        "- the maximum range of the sensor is 7cm.\n",
        "- the measured signal flattens out at a value of approximately `34`, an offset bias.\n",
        "- there is some noise in the measurement signal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VugKaszE-0T6"
      },
      "source": [
        "## Exercise 1: E-Puck Model\n",
        "\n",
        "1. If the maximum raw measurement from the `analog-to-digital` converter (ADC) is 4095, how many bits of resolution must the ADC be configured to?\n",
        "  - why might this be a different resolution to the ground sensor?\n",
        "\n",
        "2. Would you regard this sensor as:\n",
        "  - `precise`?\n",
        "  - `accurate`?\n",
        "\n",
        "3. The shape of the `sensor response` is `non-linear`, and this has implications for the robot's ability to perceive the environment:\n",
        "  - measurements below the approximate value of 306 on the y-axis suffer from **`perceptual aliaising`** - what does this mean?\n",
        "\n",
        "4. How could you normalise a sensor reading measurement to be within the range [ 0.0 : 1.0 ]?\n",
        "  - why might this be useful to do?\n",
        "\n",
        "5. What steps might you take to convert the y-axis values into units of distance?\n",
        "  - in what scenario or task would this unit conversion be useful?\n",
        "  - is this necessary for a `reactive` obstacle avoidance controller?\n",
        "\n",
        "6. What characteristic of the `sensor response` could be used to give a measure of confidence to a given sensor reading?\n",
        "  - within the context of obstacle avoidance, what does the term `confidence` refer to?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE51b9jTA9BC"
      },
      "source": [
        "<hr><br><br><br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVpp6tyQAt6k"
      },
      "source": [
        "## Obstacle Avoidance Behaviour\n",
        "\n",
        "When developing Obstacle Avoidance behaviour, it is recommended to put aside any previous line following behaviour.  It is better to simplify the problem first.  Once obstacle avoidance is achieved, the complexity of the problem can be increased to include line following behaviour.  \n",
        "\n",
        "<p align=\"center\">\n",
        "<img width=\"50%\" src=\"https://github.com/paulodowd/EMATM0053_21_22/blob/main/images/Webots_ObsAvoid.png?raw=true\">\n",
        "</p>\n",
        "\n",
        "For obstacle avoidance behaviour, it is necessary for the robot to decide:\n",
        "- is there an obstruction?\n",
        "  - a logical choice, similar to the bang-bang controller operation.\n",
        "- how to utilise the 8 sensors?\n",
        "  - a more complex issue, where a form of weighted-measurement might be useful.\n",
        "\n",
        "In Labsheet 2 we implemented a `weighted-measurement` for line following.  This was expressed as the equations:\n",
        "\n",
        "<p align=\"center\">\n",
        "$w_{left} = ( gs_{0} + ( gs_{1} * 0.5 ) )$\n",
        "</p>\n",
        "<p align=\"center\">\n",
        "$w_{right} = ( gs_{2} + ( gs_{1} * 0.5 ) )$\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "$e_{line} = w_{left} - w_{right}$\n",
        "</p>\n",
        "\n",
        "where `gs0`, `gs1`, and `gs2` correspond to the left, centre and right sensing elements of the ground sensor.  In the above equations, we can see that the value of `gs1` is weighted by 0.5.  \n",
        "\n",
        "As a matter of interest, we can also reconceptualise this technique with an illustration of the form of a very simple <a href=\"https://en.wikipedia.org/wiki/Artificial_neural_network\">artificial neural network</a>:\n",
        "\n",
        "<p align=\"center\">\n",
        "<img width=\"50%\" src=\"https://github.com/paulodowd/EMATM0053_21_22/blob/main/images/Webots_LineANN.png?raw=true\">\n",
        "</p>\n",
        "\n",
        "In the above illustration, the value from each node is fed forwards, as indicated by the arrows.  So in the first row of nodes (\"input layer\"), the nodes labelled `gs0`, `gs1`, `gs21` receive the latest value of the ground sensors.  The small numbers next to the arrows represent `weights`.  When a weight is `1.0`, the value passed forwards is unmodified.  Each node in the network operates a simple summation of all the inputs received as a `transfer function`.\n",
        "\n",
        "This representation of weighting inputs to produce a useful output is particularly useful for designing obstacle avoidance.  In the case of the e-puck robot, there are 8 infra-red proximity sensors, and we must decide how they can be used to effect obstacle avoidance behaviour. \n",
        "\n",
        "<p align=\"center\">\n",
        "<img width=\"75%\" src=\"https://github.com/paulodowd/EMATM0053_21_22/blob/main/images/Webots_ObsANN.png?raw=true\">\n",
        "</p>\n",
        "\n",
        "In the above illustration, it is necessary to decide which sensors (`ps0` to `ps7`) will be fed forwards to the output node, $e_{obs}$ (error - obstruction), and with what weighting.  Here, we are assuming a similar principle of operation, where $e_{obs}$ would be an error signal within a positive and negative range.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/cyberbotics/webots/released/docs/guide/images/robots/epuck/sensors_and_leds.png\">\n",
        "</p>\n",
        "\n",
        "If we consider the above body-plan of the e-puck robot, we can see that `ps7` and `ps0` face directly forwards.  We can imagine that encountering an obstruction on these sensors would cause a rapid turn.  Furthermore, for `ps7`, the robot might need to turn right, and for `ps0` the robot might need to turn left.  \n",
        "\n",
        "\n",
        "Within Webots, we can access the e-puck proximity sensors with the following example code extract:\n",
        "\n",
        "```c\n",
        "\n",
        "  // Loop through all proximity sensors [0:7]\n",
        "  int i;\n",
        "  for( i = 0; i < NB_PS; i++ ) {\n",
        "\n",
        "    // read value from sensor\n",
        "    ps_value[i] =  wb_distance_sensor_get_value(ps[i]);\n",
        "    \n",
        "    // Print this value to the console to inspect.\n",
        "    printf(\"%f,\", ps_value[i]);\n",
        "  }\n",
        "  printf(\"\\n\");\n",
        "```\n",
        "\n",
        "The following code extract illustrates how `weights` could be used to generate an output signal, $e_{obs}$:\n",
        "\n",
        "```c\n",
        "int i;\n",
        "// Same number of weigths as sensors\n",
        "float weights[NB_PS] = { 0.1, 0.2, 0.4, 0.5, -0.5, -0.4, -0.2, -0.1 };\n",
        "float e_obs;\n",
        "\n",
        "// Set initial value.\n",
        "e_obs = 0.0;\n",
        "for( i = 0; i < NB_PS; i++ ) {\n",
        "\n",
        "    // read value from sensor\n",
        "    ps_value[i] =  wb_distance_sensor_get_value(ps[i]);\n",
        "\n",
        "    // Simple summation of weighted sensor values.\n",
        "    e_obs = e_obs + ( ps_value[i] * weights[i] );\n",
        "\n",
        "}\n",
        "\n",
        "```\n",
        "\n",
        "After the above example code, you might utilise $e_{obs}$ in a manner similar to the Line Following exercises.  Here, it is suggested that you use $e_{obs}$ as an output signal to effect turning for your robot.  However, it is also possible to design a very simple ANN to utilise two output nodes, one to control the left motor speed and one to control the right motor speed:\n",
        "\n",
        "<p align=\"center\">\n",
        "<img width=\"50%\" src=\"https://github.com/paulodowd/EMATM0053_21_22/blob/main/images/Webots_ObsANN_2.png?raw=true\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTAQHl69vNL9"
      },
      "source": [
        "## Exercise 2: Obstacle Avoidance\n",
        "\n",
        "Depending on your implementation you could either:\n",
        " - have obstacle avoidance always active, but not not always contributing to motion (e.g., in the ANN implementation above, it is possible to have a configuration of the network that provides 0 contribution when no sensors are active).\n",
        " - have obstacle avoidance activated only when necessary.  In this method, you would need to add a simple check on your proximity sensors to decide whether obstacle avoidance is necessary.\n",
        "\n",
        "\n",
        "1. Before you start writing code consider:\n",
        "  - how could obstacle avoidance behaviour be achieved with a bang-bang controller?\n",
        "  - what advantages or disadvantages might there to a bang-bang controller?\n",
        " \n",
        "2. Decide a threshold value from the `sensor response curve` for when an `obstacle avoidance behaviour` should be activated.  \n",
        "  - **Note:** the proximity sensors always report a value, even when there is not an obstruction. \n",
        "  - write a function to check all proximity sensors and return an appropriate value if obstacle avoidance is required.\n",
        "  - check that this function operates reliably.\n",
        "  - utilise the result of this function to either:\n",
        "    - transition your FSM into/out-of obstacle avoidance appropriately (preferred)\n",
        "    - activate obstacle avoidance execution flow in your program\n",
        "\n",
        "3. Start by creating a weight of 0 for all sensors.  With just the two foward proximity sensors, find weight values to effect a turning behaviour in the approriate direction:\n",
        "  - which two proximity senors are the most forward facing?\n",
        "  - how might you pre-process your proximity sensor measurements to make subsequent calculations easier?\n",
        "  - **help:** to begin with, do not have your robot move forward or backward, only turn on the spot.  \n",
        "  - **help:** remember that you can position your robot within the environment by clicking on it.\n",
        "  - **help:** avoid **`blocking code`** - for exmaple, your obstacle avoidance function should return and be called again by `loop()` even while there is an obstruction.\n",
        "  - what would be a desirable value of `e_obs` if both forward sensors report identical values, or cancel each other out?  Implement a bias in your gain values to solve this issue.  \n",
        "    - **Hypothesise:** in what environment circumstances would the robot be in where both sensors report the same value?\n",
        "  - **Validate:** using just the two forward sensors, how far does your robot rotate away from the obstruction?  Would this enough to transit without collision?\n",
        "  - **help**: the following code extract follows a similar format to line following:\n",
        "\n",
        "\n",
        "  ```c\n",
        "\n",
        "void loop() {\n",
        "\n",
        "  \n",
        "\n",
        "  // Get the line error\n",
        "  float e_obs;\n",
        "  e_obs = getObstacleError();\n",
        "\n",
        "  // Determine a proportional rotation speed\n",
        "  float turn_velocity;\n",
        "  turn_velocity = ????;  // What is a sensible maximum speed?\n",
        "  turn_velocty = turn_velocity * e_obs;\n",
        "\n",
        "  // Set motor values.\n",
        "  // What does \"0 -\" and \"0 +\" achieve here?\n",
        "  wb_motor_set_velocity(right_motor, 0 - turn_velocity);\n",
        "  wb_motor_set_velocity(left_motor, 0 + turn_velocity);\n",
        "  \n",
        "}\n",
        "\n",
        "// A function to return an error signal representative\n",
        "// of the line placement under the ground sensor.\n",
        "float getObstacleError() {\n",
        "  float weights[NB_PS] = { 0.1, 0.2, 0.4, 0.5, -0.5, -0.4, -0.2, -0.1 };\n",
        "  float e_obs;\n",
        "\n",
        "  // Read proximity sensors, store result\n",
        "\n",
        "  // Calculated error signal\n",
        "  e_obs = ????;\n",
        "\n",
        "  // Return result\n",
        "  return e_obs;\n",
        "}\n",
        "\n",
        "```\n",
        "\n",
        "3. Progressively implement the remaining proximity sensors into your obstacle avoidance function:\n",
        "  - are all sensors necessary?\n",
        "  - two sensors are at the rear of the robot, are these useful?\n",
        "\n",
        "4. Implement a forward velocity for your obstacle avoidance:\n",
        "  - consider if this can be `proportionally controlled`, and whether this is useful.  What would be a useful measurement for forward velocity control?\n",
        "  - when would a backward velocity useful?\n",
        "\n",
        "5. How might you calculate a measure of obstacle avoidance performance?\n",
        "  - what `proprioceptive` information is available to the robot?\n",
        "  - what `exteroceptive` information is available to the robot?\n",
        "  - what features of the robot behaviour are more or less desirable, and how could these be represented mathematically within this `metric` of performance?\n",
        "  - how could this `metric` be defined and calculated to make it as transferable (comparable) as possible between different robotic systems?\n",
        "  - **help**: in machine learning, we might consider this the <a href=\"https://en.wikipedia.org/wiki/Reinforcement_learning\">reward function</a> or the <a href=\"https://en.wikipedia.org/wiki/Evolutionary_algorithm\">fitness function</a>.  \n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R505QizBBnb"
      },
      "source": [
        "<hr><br><br><br><br>"
      ]
    }
  ]
}